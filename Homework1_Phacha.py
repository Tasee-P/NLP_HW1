# -*- coding: utf-8 -*-
"""Homework1_Phacha.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lG1CfhfF2uBkpHkU3b1ROnbWIQ3WXF1C

# Setting environment and preparing url
- Library Installation (pip)
- Dependency Imports
- Resource Downloads
"""

!pip install requests

# it crash sometime so i do this command.
#!rm -rf dataset/

!pip install requests
import os
import requests
import re
import numpy as np
import pandas as pd
import nltk
import collections
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer
from collections import Counter
import matplotlib.pyplot as plt
from sklearn.feature_extraction import text
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import LatentDirichletAllocation
nltk.download('stopwords')
nltk.download('punkt_tab')

"""# Difined dataset
- Compares two different themes in novels—slavery versus romantic love stories—to see whether any interesting patterns emerge.

"""

#Set books to two fictions
books_dataset = {
    "Slave_Fiction": [
        ("TheGaries", "https://www.gutenberg.org/cache/epub/11214/pg11214.txt"),
        ("UncleTom", "https://www.gutenberg.org/cache/epub/203/pg203.txt"),
        ("Clotelle", "https://www.gutenberg.org/cache/epub/241/pg241.txt")
    ],
    "Romance_Fiction": [
        ("Emma", "https://www.gutenberg.org/cache/epub/158/pg158.txt"),
        ("Innocence", "https://www.gutenberg.org/cache/epub/541/pg541.txt"),
        ("Wuthering", "https://www.gutenberg.org/cache/epub/768/pg768.txt")
    ]
}

"""#Remove header/footer, clean the noise
 Creat function (tool) for select only body of the books.
"""

# Remove Header,Footer
# Remove Illustration tags using re.sub (Find and replace(replace with nothing("")))
# The books may be not contrain Illustration photo but just to be safe
def clean_text(text):

    if "*** START" in text:
        text = re.split(r'\*\*\* START OF.*? \*\*\*', text)[-1]
    if "*** END" in text:
        text = re.split(r'\*\*\* END OF.*? \*\*\*', text)[0]

    book_content = re.sub(r'\[Illustration.*?\]', '', text)
    return book_content.strip()

# use os.makedirs to create dataset folder (get the categories pass (slave and romance.)
# Second for loop, get url, use utf-8(for safety in case some non english writing style)
# split by the chapters to break the docs, it is most manageable way even the length of each chapter may different
# However some books, did not use 'chapters'for each section, it use I,II IV, the romance numbers, so I write if-else for both options
for categories, books_lists in books_dataset.items():
    print(f"{categories}")
    os.makedirs(f"dataset/{categories}", exist_ok=True)
    doc_count = 1

    for name, url in books_lists:
        print(f"  {name}")

        try:
            r = requests.get(url)
            r.encoding = 'utf-8'  # handles incoming data from web to Python
            raw_from_web = r.text  # use r.text to look in the link and translate the data into a readable string of characters (text)

            cleaned_content = clean_text(raw_from_web)  # calling function clean_text that we prepared above, so we get only body(content) of the books.

            if "CHAPTER" in cleaned_content.upper():
                chapters = re.split(r'(?i)^\s*CHAPTER\s+[IVXLCDM\d]+', cleaned_content, flags=re.MULTILINE)
            else:
                chapters = re.split(r'(?i)^\s*[IVXLCDM]+\.?\s*$', cleaned_content, flags=re.MULTILINE)

            # print(f"{name}: there is {len(chapters)} chapters")
            # there are too many docs much more than all original chapter from all books combine.
            # I write code to only keep real chapter/doc, using len to filter "chapter" word  from TOC or preface out
            # go to folder abd creat text file and lebel it, like Emma_1
            for chap in chapters:
                words = chap.split()

                if len(words) > 500:
                    filename = f"dataset/{categories}/{name}_{doc_count}.txt"
                    #open file, write(creat new file)
                    #f.write is equal open and paste the chapter we loop into file, clean it with. strip
                    with open(filename, "w", encoding="utf-8") as f:
                        f.write(chap.strip())
                    doc_count += 1
        except Exception as e:
            print(f" Error with {name}: {e}")

"""#How many docs we have from each categories"""

!ls dataset/Slave_Fiction | wc -l
!ls dataset/Romance_Fiction | wc -l

"""#Verify the quality of docs"""

categories = ['Slave_Fiction', 'Romance_Fiction']

for group in categories:
    folder_path = f'dataset/{group}'
    files = sorted(os.listdir(folder_path))

    print(f"\n{len(files)} files in {group}")
    small_chapters = 0
    total_words = 0

    for filename in files:
        with open(os.path.join(folder_path, filename), "r", encoding="utf-8") as f:   # We are bringing data FROM the file INTO Python+ # encoding="utf-8" tells Python how to translate the file's bytes INTO text.
            count = len(f.read().split())
            total_words += count
            if count < 500:
                print(f"{filename} is too short ({count} words)")
                small_chapters += 1

    avg = total_words / len(files) if files else 0
    print(f"Average words per file: {int(avg)}")
    print(f"Number of file <500 words: {small_chapters}")

"""# We have docs ready for NLP steps

# Text Normalization
"""

#lowered case, removed junk(numbers too), and stemmed words
# preparing tools
stemmer = PorterStemmer()
stop_words = set(stopwords.words('english'))

#lebel our books genre
categories = {
    'Slave_Fiction': 0,
    'Romance_Fiction': 1
}

alldata = []   # to hold the final clean text
fiction_group_label = [] # to hold the labels

#this just create the tools pipeline, but we did not loop to each docs yet.
def text_normalize_pipeline(raw_text):
    text = raw_text.lower()
    text = re.sub(r'[^a-z\s]', ' ', text) # this should use '' ? or ' '?, i go with ' 'it have a word like slave-hunt, shoud it be slavehunt? or slave hunt?
    words = text.split()

    # Keep the word that has more than 2 letters, this will be my experiment too, between cut 2 lens and not cut.
    clean_words = []
    for w in words:
        if w not in stop_words and len(w) > 2:
            stemmed_word = stemmer.stem(w)
            clean_words.append(stemmed_word)
    return " ".join(clean_words)

#load and process
for category, label in categories.items():
    folder_path = f"dataset/{category}"
    files = sorted(os.listdir(folder_path))

    print(f"{category} ({len(files)} docs)")

    for filename in files:
        file_path = os.path.join(folder_path, filename)

        with open(file_path, "r", encoding="utf-8") as f:
            chapter_doc_text = f.read()
            final_doc_text = text_normalize_pipeline(chapter_doc_text) #Text (docs) Normalization is happending herr, remember we created def at earlier loop
            alldata.append(final_doc_text)
            fiction_group_label.append(label)

#finaltexts are in alldata(kind of folders).
print(f"Loop through {len(alldata)} documents")
print(f"Sample Output: '{alldata[0][:200]}...'")

"""#Calculating Probabilities and Log-Likelihood"""

# use CountVectorizer counts every word in every document
# creat big bag(matrix) using fit_transform
# but we needs 2 bags, use the lebel eariler(slave=0, romance =1) for category to separate the matrix
# sum up word in each catagory
vectorizer = CountVectorizer()
X = vectorizer.fit_transform(alldata)
vocab = vectorizer.get_feature_names_out()


X_slave = X[np.array(fiction_group_label) == 0]
X_romance = X[np.array(fiction_group_label) == 1]

slave_word_counts = np.array(X_slave.sum(axis=0)).flatten()
romance_word_counts = np.array(X_romance.sum(axis=0)).flatten()

# Calculate Probabilities with Laplace Smoothing (+1)
# formula 1 :  (word count + 1) / (total words in category + Vocab Size)
# P(word | category) = (count + 1) / (total_words + vocab_size)
total_slave_words = slave_word_counts.sum()
total_romance_words = romance_word_counts.sum()
vocab_size = len(vocab)

prob_word_given_slave = (slave_word_counts + 1) / (total_slave_words + vocab_size)
prob_word_given_romance = (romance_word_counts + 1) / (total_romance_words + vocab_size)


# llr= Log(P(w|Slave) / P(w|Romance))
# Positive Score = Slave Fiction
# Negative Score = Romance Fiction
llr = np.log(prob_word_given_slave / prob_word_given_romance)

# create df to view results of llr
df = pd.DataFrame({
    'word': vocab,
    'freq_slave': slave_word_counts,
    'freq_romance': romance_word_counts,
    'llr': llr
})

# change.head to 50 or 100 if you like to see more

print(df.sort_values('llr', ascending=False).head(20)[['word', 'llr', 'freq_slave', 'freq_romance']].to_string(index=False))
print(df.sort_values('llr', ascending=True).head(20)[['word', 'llr', 'freq_slave', 'freq_romance']].to_string(index=False))

# create a list of the names I want to hide
words_to_ignore0 = [
    'clare', 'ophelia', 'eva', 'steven', 'tom', 'gari', 'elli', 'legre',
    'haley', 'sam', 'shelbi', 'cassi', 'eliza', 'clotel', 'clarenc', 'kinch', 'caddi',
    'emma', 'archer', 'heathcliff', 'weston', 'catherin', 'linton', 'elton',
    'knightley', 'olenska', 'churchil', 'fairfax', 'beaufort', 'hareton',
    'newland', 'mingott', 'hartfield', 'luyden', 'earnshaw', 'cathi', 'highburi'
]

# filter the DataFrame by show me rows where the word is NOT in the ignore list"
df_clean = df[~df['word'].isin(words_to_ignore0)]

print("Top words: Slave fiction (Some mames removed)")
print(df_clean.sort_values('llr', ascending=False).head(10)[['word', 'llr', 'freq_slave', 'freq_romance']].to_string(index=False))
print("Top words: Romance fiction (Some names removed)")
print(df_clean.sort_values('llr', ascending=True).head(10)[['word', 'llr', 'freq_slave', 'freq_romance']].to_string(index=False))

"""# LDA_Baseline"""

from sklearn.decomposition import LatentDirichletAllocation




# n_components = 4 = find me 4 distinct topics.
# random_state = 42 ensures you get the same results every time you run it
lda = LatentDirichletAllocation(n_components=4, random_state=42) # can change number 4 here to 10 if would like to see more

# fit the model to  Bag-of-Words Matrix (our metrix X)
lda.fit(X)

# for printing
def print_topic_words(model, feature_names, n_top_words):
    for topic_index, topic in enumerate(model.components_):
        message = f"Topic{topic_index}: "
        message += " ".join([feature_names[i]
                             for i in topic.argsort()[:-n_top_words - 1:-1]])
        print(message)
    print()


print("THE TOPICS")

print_topic_words(lda, vectorizer.get_feature_names_out(), 10) # can change number 10 here for longer words

"""#LDA_Experiment"""

from sklearn.feature_extraction import text # <--- IMPORT THIS

# I think we dont need name
word_ignore1 = ['clare', 'ophelia', 'eva', 'steven', 'tom', 'gari', 'elli', 'legre',
    'haley', 'sam', 'shelbi', 'cassi', 'eliza', 'clotel', 'clarenc', 'kinch',
    'caddi', 'chloe', 'topsy', 'walter', 'georg', 'henri', 'miller',
    'emma', 'archer', 'heathcliff', 'weston', 'catherin', 'linton', 'elton',
    'knightley', 'olenska', 'churchil', 'fairfax', 'beaufort', 'hareton',
    'newland', 'mingott', 'hartfield', 'luyden', 'earnshaw', 'cathi', 'highburi',
    'edgar', 'harriet', 'martin', 'hindley', 'isabella',
    'woodhous', 'jane', 'charli', 'welland', 'franke'
]


# I selected more words that cluttering in baselineoutput+some vague words.
# It could have more that you can cut like 'good', 'speak', 'talk',told','said, or may be some word i shoud not take it out like 'see', 'made' or  take it out for experiment
word_ignore2 = [
    'said', 'one', 'would', 'could', 'mr', 'miss', 'mrs', 'man', 'know',
    'like', 'well', 'come', 'see',  'made', 'littl', 'think', 'say','shall'
    'much', 'must', 'look', 'back', 'time', 'let','go', 'come','do'
]
#combine word_ignore1 and word_ignore2
word_ignore3 = ['clare', 'ophelia', 'eva', 'steven', 'tom', 'gari', 'elli', 'legre',
    'haley', 'sam', 'shelbi', 'cassi', 'eliza', 'clotel', 'clarenc', 'kinch',
    'caddi', 'chloe', 'topsy', 'walter', 'georg', 'henri', 'miller',
    'emma', 'archer', 'heathcliff', 'weston', 'catherin', 'linton', 'elton',
    'knightley', 'olenska', 'churchil', 'fairfax', 'beaufort', 'hareton',
    'newland', 'mingott', 'hartfield', 'luyden', 'earnshaw', 'cathi', 'highburi',
    'edgar', 'harriet', 'martin', 'hindley', 'isabella',
    'woodhous', 'jane', 'charli', 'welland', 'franke',
    'said', 'one', 'would', 'could', 'mr', 'miss', 'mrs', 'man', 'know',
    'like', 'well', 'come', 'see',  'made', 'littl', 'think', 'say','shall'
    'much', 'must', 'look', 'back', 'time', 'let','go', 'come','do'
]


# Union means: "Take Standard English + My Names + Fiction Words"

stop_list_names_only = list(text.ENGLISH_STOP_WORDS.union(word_ignore3)) # can chang to differnt word groups here if wnat to filter only names out
# run vectorizer with our filter list above
vectorizer_exp = CountVectorizer(stop_words=stop_list_names_only)
X_exp = vectorizer_exp.fit_transform(alldata)

lda_exp = LatentDirichletAllocation(n_components=4, random_state=42)
lda_exp.fit(X_exp)

# to print
def print_top_words(model, feature_names, n_top_words):
    for topic_index, topic in enumerate(model.components_):
        message = f"Topic {topic_index}: "
        message += " ".join([feature_names[i]
                             for i in topic.argsort()[:-n_top_words - 1:-1]])
        print(message)

# the lda experiment could be names romoves or junk removed depend how to filter the code here(stop_list_names_only = list(text.ENGLISH_STOP_WORDS.union(word_ignore3)))
# currently removed names and junked

print("LDA EXPERIMENTAL")

print_top_words(lda_exp, vectorizer_exp.get_feature_names_out(), 10) #can change number 10 here for longer words

"""#TF-IDF
(+LDA)
"""

print("TF-IDF")

my_stop_words = list(text.ENGLISH_STOP_WORDS.union(word_ignore1))
tfidf_vectorizer = TfidfVectorizer(
    stop_words=my_stop_words,
    min_df= 30,
    max_df=0.7
)

# taking alldata(243 chapters from 2 categories) and run the matrix
tfidf_matrix = tfidf_vectorizer.fit_transform(alldata)
tfidf_feature_names = tfidf_vectorizer.get_feature_names_out()

# need this to get the topics not only keywords
# tries to find patterns across all the chapters.(find theme)
lda_tfidf = LatentDirichletAllocation(n_components=4, random_state=42).
lda_tfidf.fit(tfidf_matrix)

def print_top_words(model, feature_names, n_top_words):
    for topic_index, topic in enumerate(model.components_):
        print(f"Topic {topic_index}:")
        print(" ".join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]))
    print()

print_top_words(lda_tfidf, tfidf_feature_names, 10)

# ai help me here on weight output.
def print_top_words_with_scores(model, feature_names, n_top_words):
    for topic_index, topic in enumerate(model.components_):
        print(f"\nTopic {topic_index}:")
        # get the top indices just like before
        top_indices = topic.argsort()[:-n_top_words - 1:-1]

        # loop through and print the word and the score
        for i in top_indices:
            word = feature_names[i]
            score = topic[i]
            print(f"  {word}: {score:.2f}")


print_top_words_with_scores(lda_tfidf, tfidf_feature_names, 10)